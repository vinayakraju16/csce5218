{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vYiZq0X2oB5t"
      },
      "source": [
        "# **CSCE 5218 / CSCE 4930 Deep Learning**\n",
        "\n",
        "# **The Perceptron** (20 pt)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vGVmKzgG2Ium",
        "outputId": "28fd12c4-3b2e-4f1c-ad40-32e42b9752c9"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current',\n",
              " '                                 Dload  Upload   Total   Spent    Left  Speed',\n",
              " '',\n",
              " '  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0',\n",
              " '100 11645  100 11645    0     0  56786      0 --:--:-- --:--:-- --:--:-- 57083']"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "# Get the datasets\n",
        "!!/usr/bin/curl --output test.dat https://raw.githubusercontent.com/huangyanann/CSCE5218/main/test_small.txt\n",
        "!!/usr/bin/curl --output train.dat https://raw.githubusercontent.com/huangyanann/CSCE5218/main/train.txt\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A69DxPSc8vNs",
        "outputId": "7fa7b7a9-75ff-4f96-b728-d9c8247c09dd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A1\tA2\tA3\tA4\tA5\tA6\tA7\tA8\tA9\tA10\tA11\tA12\tA13\t\r\n",
            "1\t1\t0\t0\t0\t0\t0\t0\t1\t1\t0\t0\t1\t0\r\n",
            "0\t0\t1\t1\t0\t1\t1\t0\t0\t0\t0\t0\t1\t0\r\n",
            "0\t1\t0\t1\t1\t0\t1\t0\t1\t1\t1\t0\t1\t1\r\n",
            "0\t0\t1\t0\t0\t1\t0\t1\t0\t1\t1\t1\t1\t0\r\n",
            "0\t1\t0\t0\t0\t0\t0\t1\t1\t1\t1\t1\t1\t0\r\n",
            "0\t1\t1\t1\t0\t0\t0\t1\t0\t1\t1\t0\t1\t1\r\n",
            "0\t1\t1\t0\t0\t0\t1\t0\t0\t0\t0\t0\t1\t0\r\n",
            "0\t0\t0\t1\t1\t0\t1\t1\t1\t0\t0\t0\t1\t0\r\n",
            "0\t0\t0\t0\t0\t0\t1\t0\t1\t0\t1\t0\t1\t0\n",
            "X1\tX2\tX3\n",
            "1\t1\t1\t1\n",
            "0\t0\t1\t1\n",
            "0\t1\t1\t0\n",
            "0\t1\t1\t0\n",
            "0\t1\t1\t0\n",
            "0\t1\t1\t0\n",
            "0\t1\t1\t0\n",
            "0\t1\t1\t0\n",
            "1\t1\t1\t1\n"
          ]
        }
      ],
      "source": [
        "# Take a peek at the datasets\n",
        "!head train.dat\n",
        "!head test.dat"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rFXHLhnhwiBR"
      },
      "source": [
        "### Build the Perceptron Model\n",
        "\n",
        "You will need to complete some of the function definitions below.  DO NOT import any other libraries to complete this."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "cXAsP_lw3QwJ"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import itertools\n",
        "import re\n",
        "\n",
        "# Corpus reader, all columns but the last one are coordinates;\n",
        "#   the last column is the label\n",
        "\n",
        "def read_data(file_name):\n",
        "    f = open(file_name, 'r')\n",
        "\n",
        "    data = []\n",
        "    # Discard header line\n",
        "    f.readline()\n",
        "    for instance in f.readlines():\n",
        "        if not re.search('\\t', instance): continue\n",
        "        instance = list(map(int, instance.strip().split('\\t')))\n",
        "        # Add a dummy input so that w0 becomes the bias\n",
        "        instance = [-1] + instance\n",
        "        data += [instance]\n",
        "    return data\n",
        "\n",
        "\n",
        "def dot_product(array1, array2):\n",
        "    # Return dot product of array 1 and array 2\n",
        "    return sum([a * b for a, b in zip(array1, array2)])\n",
        "\n",
        "\n",
        "def sigmoid(x):\n",
        "    # Return output of sigmoid function on x\n",
        "    return 1 / (1 + math.exp(-x))\n",
        "\n",
        "\n",
        "# The output of the model, which for the perceptron is\n",
        "# the sigmoid function applied to the dot product of\n",
        "# the instance and the weights\n",
        "\n",
        "def output(weight, instance):\n",
        "    # Return the output of the model\n",
        "    return sigmoid(dot_product(weight, instance))\n",
        "\n",
        "\n",
        "# Predict the label of an instance; this is the definition of the perceptron\n",
        "# you should output 1 if the output is >= 0.5 else output 0\n",
        "\n",
        "def predict(weights, instance):\n",
        "    # Return the prediction of the model\n",
        "    return 1 if output(weights, instance) >= 0.5 else 0\n",
        "\n",
        "\n",
        "# Accuracy = percent of correct predictions\n",
        "\n",
        "def get_accuracy(weights, instances):\n",
        "    correct = sum([1 if predict(weights, instance) == instance[-1] else 0\n",
        "                   for instance in instances])\n",
        "    return correct * 100 / len(instances)\n",
        "\n",
        "\n",
        "# Train a perceptron with instances and hyperparameters:\n",
        "#       lr (learning rate)\n",
        "#       epochs\n",
        "\n",
        "def train_perceptron(instances, lr, epochs):\n",
        "\n",
        "    # Initialize weights with zeros\n",
        "    weights = [0] * (len(instances[0])-1)\n",
        "\n",
        "    for _ in range(epochs):\n",
        "        for instance in instances:\n",
        "            # Calculate the dot product\n",
        "            in_value = dot_product(weights, instance)\n",
        "            # Apply the sigmoid function\n",
        "            output_value = sigmoid(in_value)\n",
        "            # Calculate the error\n",
        "            error = instance[-1] - output_value\n",
        "            # Update weights\n",
        "            for i in range(len(weights)):\n",
        "                weights[i] += lr * error * output_value * (1-output_value) * instance[i]\n",
        "\n",
        "    return weights"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "adBZuMlAwiBT"
      },
      "source": [
        "## Run it"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "50YvUza-BYQF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d0ef63ec-abdb-4e08-a753-9197751dc555"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "#tr: 400, epochs:   5, learning rate: 0.005; Accuracy (test, 14 instances): 71.4\n"
          ]
        }
      ],
      "source": [
        "instances_tr = read_data(\"train.dat\")\n",
        "instances_te = read_data(\"test.dat\")\n",
        "lr = 0.005\n",
        "epochs = 5\n",
        "weights = train_perceptron(instances_tr, lr, epochs)\n",
        "accuracy = get_accuracy(weights, instances_te)\n",
        "print(f\"#tr: {len(instances_tr):3}, epochs: {epochs:3}, learning rate: {lr:.3f}; \"\n",
        "      f\"Accuracy (test, {len(instances_te)} instances): {accuracy:.1f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CBXkvaiQMohX"
      },
      "source": [
        "## Questions\n",
        "\n",
        "Answer the following questions. Include your implementation and the output for each question."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YCQ6BEk1CBlr"
      },
      "source": [
        "\n",
        "\n",
        "### Question 1\n",
        "\n",
        "In `train_perceptron(instances, lr, epochs)`, we have the follosing code:\n",
        "```\n",
        "in_value = dot_product(weights, instance)\n",
        "output = sigmoid(in_value)\n",
        "error = instance[-1] - output\n",
        "```\n",
        "\n",
        "Why don't we have the following code snippet instead?\n",
        "```\n",
        "output = predict(weights, instance)\n",
        "error = instance[-1] - output\n",
        "```\n",
        "\n",
        "#### TODO Add your answer here (text only)\n",
        "\n",
        "# Answer for Question 1\n",
        "\n",
        "### The reason we use in_value = dot_product(weights, instance) followed by output = sigmoid(in_value) instead of using output = predict(weights, instance) is that during training, we need the raw output of the sigmoid function (a probability) rather than a binary prediction (0 or 1). The sigmoid function provides a smooth gradient,which is necessary for calculating the error and updating weights through gradient descent.Using predict() would give us a discrete output (0 or 1), losing the gradient information needed for learning.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JU3c3m6YL2rK"
      },
      "source": [
        "### Question 2\n",
        "Train the perceptron with the following hyperparameters and calculate the accuracy with the test dataset.\n",
        "\n",
        "```\n",
        "tr_percent = [5, 10, 25, 50, 75, 100] # percent of the training dataset to train with\n",
        "num_epochs = [5, 10, 20, 50, 100]              # number of epochs\n",
        "lr = [0.005, 0.01, 0.05]              # learning rate\n",
        "```\n",
        "\n",
        "TODO: Write your code below and include the output at the end of each training loop (NOT AFTER EACH EPOCH)\n",
        "of your code.The output should look like the following:\n",
        "```\n",
        "# tr:  20, epochs:   5, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
        "# tr:  20, epochs:  10, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
        "# tr:  20, epochs:  20, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
        "[and so on for all the combinations]\n",
        "```\n",
        "You will get different results with different hyperparameters.\n",
        "\n",
        "#### TODO Add your answer here (code and output in the format above)\n",
        "\n",
        "# Answer for Question 2\n",
        "tr_percent = [5, 10, 25, 50, 75, 100]  # percent of the training dataset to train with\n",
        "num_epochs = [5, 10, 20, 50, 100]      # number of epochs\n",
        "lr = [0.005, 0.01, 0.05]               # learning rate\n",
        "\n",
        "# Training and evaluating the perceptron model\n",
        "def evaluate_perceptron(train_data, test_data):\n",
        "    total_train_instances = len(train_data)\n",
        "    test_instances = len(test_data)\n",
        "\n",
        "    for tr in tr_percent:\n",
        "        num_train = int((tr / 100) * total_train_instances)\n",
        "        training_subset = train_data[:num_train]\n",
        "        for epochs in num_epochs:\n",
        "            for learning_rate in lr:\n",
        "                weights = train_perceptron(training_subset, learning_rate, epochs)\n",
        "                accuracy = get_accuracy(weights, test_data)\n",
        "                print(f\"# tr: {num_train}, epochs: {epochs}, learning rate: {learning_rate:.3f}; Accuracy (test, {test_instances} instances): {accuracy:.1f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "G-VKJOUu2BTp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b81ec1d8-f359-4be8-ee57-1f6df8ba338c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "#tr: 20, epochs: 100, learning rate: 0.005; Accuracy (test, 14 instances): 85.7\n",
            "#tr: 40, epochs: 100, learning rate: 0.005; Accuracy (test, 14 instances): 71.4\n",
            "#tr: 100, epochs: 100, learning rate: 0.005; Accuracy (test, 14 instances): 71.4\n",
            "#tr: 200, epochs: 100, learning rate: 0.005; Accuracy (test, 14 instances): 85.7\n",
            "#tr: 300, epochs: 100, learning rate: 0.005; Accuracy (test, 14 instances): 85.7\n",
            "#tr: 400, epochs: 100, learning rate: 0.005; Accuracy (test, 14 instances): 71.4\n",
            "#tr: 20, epochs: 100, learning rate: 0.010; Accuracy (test, 14 instances): 42.9\n",
            "#tr: 40, epochs: 100, learning rate: 0.010; Accuracy (test, 14 instances): 85.7\n",
            "#tr: 100, epochs: 100, learning rate: 0.010; Accuracy (test, 14 instances): 28.6\n",
            "#tr: 200, epochs: 100, learning rate: 0.010; Accuracy (test, 14 instances): 85.7\n",
            "#tr: 300, epochs: 100, learning rate: 0.010; Accuracy (test, 14 instances): 85.7\n",
            "#tr: 400, epochs: 100, learning rate: 0.010; Accuracy (test, 14 instances): 71.4\n",
            "#tr: 20, epochs: 100, learning rate: 0.050; Accuracy (test, 14 instances): 42.9\n",
            "#tr: 40, epochs: 100, learning rate: 0.050; Accuracy (test, 14 instances): 42.9\n",
            "#tr: 100, epochs: 100, learning rate: 0.050; Accuracy (test, 14 instances): 28.6\n",
            "#tr: 200, epochs: 100, learning rate: 0.050; Accuracy (test, 14 instances): 85.7\n",
            "#tr: 300, epochs: 100, learning rate: 0.050; Accuracy (test, 14 instances): 85.7\n",
            "#tr: 400, epochs: 100, learning rate: 0.050; Accuracy (test, 14 instances): 71.4\n"
          ]
        }
      ],
      "source": [
        "instances_tr = read_data(\"train.dat\")\n",
        "instances_te = read_data(\"test.dat\")\n",
        "tr_percent = [5, 10, 25, 50, 75, 100] # percent of the training dataset to train with\n",
        "num_epochs = [5, 10, 20, 50, 100]     # number of epochs\n",
        "lr_array = [0.005, 0.01, 0.05]        # learning rate\n",
        "\n",
        "for lr in lr_array:\n",
        "  for tr_size in tr_percent:\n",
        "    for epochs in num_epochs:\n",
        "      size =  round(len(instances_tr)*tr_size/100)\n",
        "      pre_instances = instances_tr[0:size]\n",
        "      weights = train_perceptron(pre_instances, lr, epochs)\n",
        "      accuracy = get_accuracy(weights, instances_te)\n",
        "    print(f\"#tr: {len(pre_instances):0}, epochs: {epochs:3}, learning rate: {lr:.3f}; \"\n",
        "            f\"Accuracy (test, {len(instances_te)} instances): {accuracy:.1f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OFB9MtwML24O"
      },
      "source": [
        "### Question 3\n",
        "Write a couple paragraphs interpreting the results with all the combinations of hyperparameters. Drawing a plot will probably help you make a point. In particular, answer the following:\n",
        "- A. Do you need to train with all the training dataset to get the highest accuracy with the test dataset?\n",
        "- B. How do you justify that training the second run obtains worse accuracy than the first one (despite the second one uses more training data)?\n",
        "   ```\n",
        "#tr: 100, epochs:  20, learning rate: 0.050; Accuracy (test, 100 instances): 71.0\n",
        "#tr: 200, epochs:  20, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
        "```\n",
        "- C. Can you get higher accuracy with additional hyperparameters (higher than `80.0`)?\n",
        "- D. Is it always worth training for more epochs (while keeping all other hyperparameters fixed)?\n",
        "\n",
        "#### TODO: Add your answer here (code and text)\n",
        "\n",
        "# Answer for Question 3\n",
        "## Answer A:\n",
        "### It is not always necessary to use the entire training dataset to achieve the highest accuracy on the test dataset. Sometimes, using a smaller subset of the training data with optimal learning rates and epochs can provide comparable or even better performance.\n",
        "\n",
        "## Answer B:\n",
        "### The observed decrease in accuracy when using more training data with the same hyperparameters\n",
        "### (e.g.,  \n",
        "### tr: 100, epochs: 20, learning rate: 0.050; Accuracy: 71.0 vs.\n",
        "### tr: 200, epochs: 20, learning rate: 0.005; Accuracy: 68.0)\n",
        "### can be attributed to underfitting. When the learning rate is too low, the model may not learn enough from the increased data, leading to suboptimal performance.\n",
        "\n",
        "## Answer C:\n",
        "### Additional hyperparameters, such as a higher learning rate or more epochs, could potentially push the accuracy beyond 80%. However, this would require careful tuning to avoid overfitting.\n",
        "\n",
        "## Answer D:\n",
        "### Training for more epochs is not always beneficial. While it might help the model converge, excessive epochs with a fixed learning rate and data size can lead to overfitting, reducing performance on the test set.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "38rA_Kp3wiBX"
      },
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}